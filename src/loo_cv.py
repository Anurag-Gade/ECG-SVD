# -*- coding: utf-8 -*-
"""LOO CV

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wp9KnwM8hEZbaJN6r2MOibCvTbPBSXbM
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.metrics import classification_report,confusion_matrix
import cv2
import os
from PIL import Image
from tqdm.notebook import tqdm
import glob
from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut
from sklearn import metrics
import imblearn
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.utils import plot_model
import warnings
import statistics
import gc
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Research - BITS Pilani/ECG/source
# %cd /content/drive/MyDrive/ECG/source

seed = 420
np.random.seed(420)

def get_arrays():
  X = []
  y = []
  images = "../data/PTB Database/OneWindow/Images/OneWindow_Healthy"
  datapath = os.path.join(images,'*g') 
  files = sorted(glob.glob(datapath))
  # print(files)
  # files.sort(key = lambda x: int(x.split("(")[-1].split(")")[0]))
  # print(files)
  # return 
  for f in tqdm(files):
    a=np.asarray((Image.open(f)).resize((128,128)))
    a=cv2.cvtColor(a,cv2.COLOR_RGBA2RGB)
    X.append(a)
    y.append(0)
  images = "../data/PTB Database/OneWindow/Images/OneWindow_MI"
  datapath = os.path.join(images,'*g')
  files = sorted(glob.glob(datapath))
  # files.sort(key = lambda x: int(x.split("(")[-1].split(")")[0])) 
  for f in tqdm(files):
    a=np.asarray((Image.open(f)).resize((128,128)))
    a=cv2.cvtColor(a,cv2.COLOR_RGBA2RGB)
    X.append(a)
    y.append(1)
  
  X = np.array(X)
  y = np.array(y) 

  return X,y

X, y = get_arrays()

X.shape, y.shape

def get_SVD_arrays():
  X = []
  y = []

  #Normal
  # npy_folder = "/content/drive/MyDrive/Research - BITS Pilani/ECG/data/SVD_Images/Normal Person ECG Images (284x12=3408)"
  npy_folder = "../data/PTB Database/OneWindow/SVD/OneWindow_Healthy"
  datapath = os.path.join(npy_folder,'*y') 
  files = sorted(glob.glob(datapath))
  # files.sort(key = lambda x: int(x.split("(")[-1].split(")")[0]))
  for f in tqdm(files):
    npy_array = np.load(f) 
    X.append(npy_array) 
    y.append(0)

  # npy_folder = "/content/drive/MyDrive/Research - BITS Pilani/ECG/data/SVD_Images/ECG Images of Myocardial Infarction Patients (240x12=2880)"
  npy_folder = "../data/PTB Database/OneWindow/SVD/OneWindow_MI"
  datapath = os.path.join(npy_folder,'*y')
  files = sorted(glob.glob(datapath))
  # files.sort(key = lambda x: int(x.split("(")[-1].split(")")[0]))
  for f in tqdm(files):
    npy_array = np.load(f) 
    X.append(npy_array) 
    y.append(1)
  
  #Converting the list into a numpy array
  X = np.array(X)
  y = np.array(y) 

  return X,y

X_SVD,y_SVD=get_SVD_arrays()

X_SVD.shape, y_SVD.shape

def build_effnet_model():
    """
    Build the EfficientNetV2B2 model
    """
    
    input_image = layers.Input(shape=(128, 128, 3))
    input_ch0 = layers.Input(shape=(128, 128, 3))
    input_ch1 = layers.Input(shape=(128, 128, 3))
    input_ch2 = layers.Input(shape=(128, 128, 3))
    input_ch3 = layers.Input(shape=(128, 128, 3))
    input_ch4 = layers.Input(shape=(128, 128, 3))

    tl_img = EfficientNetV2B2(include_top=False, input_tensor=input_image, weights="imagenet")
    tl_ch0 = EfficientNetV2B2(include_top=False, input_tensor=input_ch0, weights="imagenet")
    tl_ch1 = EfficientNetV2B2(include_top=False, input_tensor=input_ch1, weights="imagenet")
    tl_ch2 = EfficientNetV2B2(include_top=False, input_tensor=input_ch2, weights="imagenet")
    tl_ch3 = EfficientNetV2B2(include_top=False, input_tensor=input_ch3, weights="imagenet")
    tl_ch4 = EfficientNetV2B2(include_top=False, input_tensor=input_ch4, weights="imagenet")

    for layer in tl_img.layers:
      layer._name = layer._name + str("_img")
    for layer in tl_ch0.layers:
      layer._name = layer._name + str("_ch0")
    for layer in tl_ch1.layers:
      layer._name = layer._name + str("_ch1")
    for layer in tl_ch2.layers:
      layer._name = layer._name + str("_ch2")
    for layer in tl_ch3.layers:
      layer._name = layer._name + str("_ch3")
    for layer in tl_ch4.layers:
      layer._name = layer._name + str("_ch4")

      

    # Freeze the pretrained weights
    tl_img.trainable = False
    tl_ch0.trainable = False
    tl_ch1.trainable = False
    tl_ch2.trainable = False
    tl_ch3.trainable = False
    tl_ch4.trainable = False

    # Rebuild top
    gap_img = layers.GlobalAveragePooling2D(name="avg_pool_img")(tl_img.output)
    gap_ch0 = layers.GlobalAveragePooling2D(name="avg_pool_ch0")(tl_ch0.output)
    gap_ch1 = layers.GlobalAveragePooling2D(name="avg_pool_ch1")(tl_ch1.output)
    gap_ch2 = layers.GlobalAveragePooling2D(name="avg_pool_ch2")(tl_ch2.output)
    gap_ch3 = layers.GlobalAveragePooling2D(name="avg_pool_ch3")(tl_ch3.output)
    gap_ch4 = layers.GlobalAveragePooling2D(name="avg_pool_ch4")(tl_ch4.output)

    bn_img = layers.BatchNormalization(name="bn_img")(gap_img)
    bn_ch0 = layers.BatchNormalization(name="bn_ch0")(gap_ch0)
    bn_ch1 = layers.BatchNormalization(name="bn_ch1")(gap_ch1)
    bn_ch2 = layers.BatchNormalization(name="bn_ch2")(gap_ch2)
    bn_ch3 = layers.BatchNormalization(name="bn_ch3")(gap_ch3)
    bn_ch4 = layers.BatchNormalization(name="bn_ch4")(gap_ch4)

    drop_img = layers.Dropout(0.1, name="dropout_img")(bn_img)
    drop_ch0 = layers.Dropout(0.15, name="dropout_ch0")(bn_ch0)
    drop_ch1 = layers.Dropout(0.15, name="dropout_ch1")(bn_ch1)
    drop_ch2 = layers.Dropout(0.15, name="dropout_ch2")(bn_ch2)
    drop_ch3 = layers.Dropout(0.15, name="dropout_ch3")(bn_ch3)
    drop_ch4 = layers.Dropout(0.15, name="dropout_ch4")(bn_ch4)

    logits_img = layers.Dense(3, name="logits_img", activation="softmax")(drop_img)
    logits_ch0 = layers.Dense(3, name="logits_ch0", activation="softmax")(drop_ch0)
    logits_ch1 = layers.Dense(3, name="logits_ch1", activation="softmax")(drop_ch1)
    logits_ch2 = layers.Dense(3, name="logits_ch2", activation="softmax")(drop_ch2)
    logits_ch3 = layers.Dense(3, name="logits_ch3", activation="softmax")(drop_ch3)
    logits_ch4 = layers.Dense(3, name="logits_ch4", activation="softmax")(drop_ch4)
    

    concat = layers.Concatenate()([logits_img, logits_ch0, logits_ch1, logits_ch2, logits_ch3, logits_ch4])

    hidden = layers.Dense(10, activation='relu')(concat)
    bn = layers.BatchNormalization()(hidden)
    output = layers.Dense(2, activation='softmax')(hidden)

    model = Model(
        inputs=[input_image, input_ch0, input_ch1, input_ch2, input_ch3, input_ch4],
        outputs = output,
        name="DRL"
    )

    # model.compile(optimizer=optimizer, loss="sparse_categorical_crossentropy", metrics=["accuracy"])

    return model

loss_list = []
acc_list = []

loo = LeaveOneOut()
for i, (train_index, test_index) in enumerate(loo.split(X)):
  print(f"Fold {i}, Test index {test_index}")
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]
  X_SVD_train, X_SVD_test = X_SVD[train_index], X_SVD[test_index]

  X_tr_ch0 = np.repeat(X_SVD_train[:,:,:,0, np.newaxis], 3, -1)
  X_tr_ch1 = np.repeat(X_SVD_train[:,:,:,1, np.newaxis], 3, -1)
  X_tr_ch2 = np.repeat(X_SVD_train[:,:,:,2, np.newaxis], 3, -1)
  X_tr_ch3 = np.repeat(X_SVD_train[:,:,:,3, np.newaxis], 3, -1)
  X_tr_ch4 = np.repeat(X_SVD_train[:,:,:,4, np.newaxis], 3, -1)

  X_test_ch0 = np.repeat(X_SVD_test[:,:,:,0, np.newaxis], 3, -1)
  X_test_ch1 = np.repeat(X_SVD_test[:,:,:,1, np.newaxis], 3, -1)
  X_test_ch2 = np.repeat(X_SVD_test[:,:,:,2, np.newaxis], 3, -1)
  X_test_ch3 = np.repeat(X_SVD_test[:,:,:,3, np.newaxis], 3, -1)
  X_test_ch4 = np.repeat(X_SVD_test[:,:,:,4, np.newaxis], 3, -1)

  tf.keras.backend.clear_session()
  optimizer = tf.keras.optimizers.Adam(learning_rate=7.5e-3)
  model=build_effnet_model()
  model.compile(optimizer=optimizer, loss="sparse_categorical_crossentropy", metrics=["accuracy"])

  hist=model.fit(
      x=[X_train,X_tr_ch0,X_tr_ch1,X_tr_ch2,X_tr_ch3,X_tr_ch4],
      y=y_train,
      epochs=40,
      batch_size=16,
      validation_data=([X_test,X_test_ch0,X_test_ch1,X_test_ch2,X_test_ch3,X_test_ch4],y_test)
  )

  loss, acc = model.evaluate([X_test,X_test_ch0,X_test_ch1,X_test_ch2,X_test_ch3,X_test_ch4],y_test)
  print(f"Loss = {loss}, Accuracy = {acc}")
  loss_list.append(loss)
  acc_list.append(acc)

  # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_SVD_train.shape, X_SVD_test.shape, X_tr_ch0.shape, X_test_ch1.shape)
  gc.collect()
  # if i==4:
  #   break

print(f"Average Accuracy = {np.sum(acc_list)/len(acc_list)}")

acc_list

